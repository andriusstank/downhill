{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Downhill Reverse mode automatic differentiation in Haskell style. Overview Downhill library proposes an approach to automatic differentiation that is both well typed and simple. Like Backprop , it allows variables to have different types. Traditionally reverse mode automatic differentiation works by constructing computational graph or Wengert list. However, faithfully representing heterogeneous graph in Haskell would result in horrible types. This library exploits linearity of derivative and constructs linear computational graph to keep everything simple. Bringing heterogeneity to linear graphs don\u2019t make them more complicated. It makes working with them much easier, thanks to parametric polymorphism. Related work Backprop Backprop introduced heterogeneous reverse mode differentiation as a usable library. It has a well typed and nice interface, but it relies on rather complicated machinery under the hood. Downhill library explores alternative ways to implement automatic differentiation. Conal Elliott\u2019s paper The Simple Essence of Automatic Differentiation by Conal Elliott explains how gradient is a linear map and all chain rules is just compositions of linear maps. The idea of linear graphs was inspired by this paper.","title":"Home"},{"location":"#downhill","text":"Reverse mode automatic differentiation in Haskell style.","title":"Downhill"},{"location":"#overview","text":"Downhill library proposes an approach to automatic differentiation that is both well typed and simple. Like Backprop , it allows variables to have different types. Traditionally reverse mode automatic differentiation works by constructing computational graph or Wengert list. However, faithfully representing heterogeneous graph in Haskell would result in horrible types. This library exploits linearity of derivative and constructs linear computational graph to keep everything simple. Bringing heterogeneity to linear graphs don\u2019t make them more complicated. It makes working with them much easier, thanks to parametric polymorphism.","title":"Overview"},{"location":"#related-work","text":"","title":"Related work"},{"location":"#backprop","text":"Backprop introduced heterogeneous reverse mode differentiation as a usable library. It has a well typed and nice interface, but it relies on rather complicated machinery under the hood. Downhill library explores alternative ways to implement automatic differentiation.","title":"Backprop"},{"location":"#conal-elliotts-paper","text":"The Simple Essence of Automatic Differentiation by Conal Elliott explains how gradient is a linear map and all chain rules is just compositions of linear maps. The idea of linear graphs was inspired by this paper.","title":"Conal Elliott's paper"},{"location":"getstart/","text":"Get Started There\u2019s no user guide, sorry. In the meantime see samples directory in the source code repository. Also keep in mind that BVar , BackGrad and all their parts are exposed by the library. You are encouraged to build variables yourself to add any functions or data types you need.","title":"Get Started"},{"location":"getstart/#get-started","text":"There\u2019s no user guide, sorry. In the meantime see samples directory in the source code repository. Also keep in mind that BVar , BackGrad and all their parts are exposed by the library. You are encouraged to build variables yourself to add any functions or data types you need.","title":"Get Started"},{"location":"intro/part1/","text":"On the type of the gradient We focus on differentiating a scalar valued function f :: V -> R , where V is a vector space over real numbers R . Reverse mode automatic differentiation computes gradients for all intermediate variables. The gradient is usually given the same type as the variable itself. This is obvious thing to do when you see all variables as numbers or arrays of numbers. Yet we can do better by making good use of the type system. Mixing variables and gradients generally makes no sense. Code is more readable and easier to write if we give them different types. As we flip the edges of computational graph, variables and gradients kind of swap roles. That would be incredibly confusing if compiler didn\u2019t help tracking which is which. Dimensional analysis Units make a great example of why we might want to keep distinction between variables and their gradients. Take an expression to compute mass ratio of a rocket as example: dry, fuel :: Kg ratio = (dry+fuel) / dry :: R Here dry is the mass of empty rocket, fuel is the mass of the fuel, both measured in kilograms. Mass ratio, as well as its gradient, are dimensionless real numbers. It was intentionally chosen so, in order to avoid circular reasoning. Let\u2019s compute gradients of dry and fuel , starting backpropagation from ratio : d_ratio = 1 :: R d_fuel = 1/dry :: Kg^(-1) d_dry = -fuel/dry^2 :: Kg^(-1) See? Variables have units of \\(\\mathrm{kg}\\) , while their gradients are measured in \\(\\frac{1}{\\mathrm{kg}}\\) . That\u2019s because variables are contravariant vectors, while gradients are covariant vectors. Their units are inverse of each other. Superficial Hilbert space Use of the same type V for both variables gradients is typically justified by Riesz representation theorem. Gradient of a variable x :: V is fundamentally a linear function of type V -> R . Of course, such a representation is unworkable \u2013 we need to store gradients as numbers and actually compute sums during reverse accumulation. Otherwise we will run into combinatorial explosion of repeated computations. Now if V happens to be Hilbert space, then V -> R is isomorphic to V . For our purposes Hilbert space requirement boils down to existance of a well behaved inner product (<.>) :: V -> V -> R . Seems to be a benign requirement \u2013 any finite dimensional vector space over real numbers can be equipped with one. We have a solid theory behind using V type for gradients. Nonetheless, we saw units don\u2019t match. How come? Turns out we can\u2019t take inner product for granted. Record with variety of units shows this clearly: data Rocket = Rocket { rocketMass :: Kg , rocketVelocity :: Meter/Second } -- InnerSpace comes from vector-space package. instance InnerSpace Rocket where Rocket m1 v1 <.> Rocket m2 v2 = (m1 <.> m2) ^+^ (v1 <.> v2) We are adding \\(\\frac{m^2}{s^2}\\) to \\(\\mathrm{kg}^2\\) and expecting to get a dimensionless scalar! Although it is possible to provide an InnerSpace Rocket instance \u2013 all we need to do is to drop units \u2013 but we\u2019d very much rather not to. Isomorphism grounded on such an instance is akin to isomorphism between kilograms and meters \u2013 it does indeed exist, but we wouldn\u2019t want to invoke it automatically. Gradient descent Having established the rule of no mixing of variables and gradients, we see that gradient descent is in outright violation of this rule: $$ \\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n) $$ We need a metric tensor to relate gradients and variables here. It\u2019s logical when you think about it \u2013 gradient descent moves in the direction of the steepest descent. Ability to measure distances is needed to make any sense of steepness. Metric tensor brings different units into commensurable quantities. It also plays the role of preconditioner. Therefore, we opt to passing metric tensor explicitly instead of demanding existance of canonical one via Hilber space.","title":"Gradient Type"},{"location":"intro/part1/#on-the-type-of-the-gradient","text":"We focus on differentiating a scalar valued function f :: V -> R , where V is a vector space over real numbers R . Reverse mode automatic differentiation computes gradients for all intermediate variables. The gradient is usually given the same type as the variable itself. This is obvious thing to do when you see all variables as numbers or arrays of numbers. Yet we can do better by making good use of the type system. Mixing variables and gradients generally makes no sense. Code is more readable and easier to write if we give them different types. As we flip the edges of computational graph, variables and gradients kind of swap roles. That would be incredibly confusing if compiler didn\u2019t help tracking which is which.","title":"On the type of the gradient"},{"location":"intro/part1/#dimensional-analysis","text":"Units make a great example of why we might want to keep distinction between variables and their gradients. Take an expression to compute mass ratio of a rocket as example: dry, fuel :: Kg ratio = (dry+fuel) / dry :: R Here dry is the mass of empty rocket, fuel is the mass of the fuel, both measured in kilograms. Mass ratio, as well as its gradient, are dimensionless real numbers. It was intentionally chosen so, in order to avoid circular reasoning. Let\u2019s compute gradients of dry and fuel , starting backpropagation from ratio : d_ratio = 1 :: R d_fuel = 1/dry :: Kg^(-1) d_dry = -fuel/dry^2 :: Kg^(-1) See? Variables have units of \\(\\mathrm{kg}\\) , while their gradients are measured in \\(\\frac{1}{\\mathrm{kg}}\\) . That\u2019s because variables are contravariant vectors, while gradients are covariant vectors. Their units are inverse of each other.","title":"Dimensional analysis"},{"location":"intro/part1/#superficial-hilbert-space","text":"Use of the same type V for both variables gradients is typically justified by Riesz representation theorem. Gradient of a variable x :: V is fundamentally a linear function of type V -> R . Of course, such a representation is unworkable \u2013 we need to store gradients as numbers and actually compute sums during reverse accumulation. Otherwise we will run into combinatorial explosion of repeated computations. Now if V happens to be Hilbert space, then V -> R is isomorphic to V . For our purposes Hilbert space requirement boils down to existance of a well behaved inner product (<.>) :: V -> V -> R . Seems to be a benign requirement \u2013 any finite dimensional vector space over real numbers can be equipped with one. We have a solid theory behind using V type for gradients. Nonetheless, we saw units don\u2019t match. How come? Turns out we can\u2019t take inner product for granted. Record with variety of units shows this clearly: data Rocket = Rocket { rocketMass :: Kg , rocketVelocity :: Meter/Second } -- InnerSpace comes from vector-space package. instance InnerSpace Rocket where Rocket m1 v1 <.> Rocket m2 v2 = (m1 <.> m2) ^+^ (v1 <.> v2) We are adding \\(\\frac{m^2}{s^2}\\) to \\(\\mathrm{kg}^2\\) and expecting to get a dimensionless scalar! Although it is possible to provide an InnerSpace Rocket instance \u2013 all we need to do is to drop units \u2013 but we\u2019d very much rather not to. Isomorphism grounded on such an instance is akin to isomorphism between kilograms and meters \u2013 it does indeed exist, but we wouldn\u2019t want to invoke it automatically.","title":"Superficial Hilbert space"},{"location":"intro/part1/#gradient-descent","text":"Having established the rule of no mixing of variables and gradients, we see that gradient descent is in outright violation of this rule: $$ \\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n) $$ We need a metric tensor to relate gradients and variables here. It\u2019s logical when you think about it \u2013 gradient descent moves in the direction of the steepest descent. Ability to measure distances is needed to make any sense of steepness. Metric tensor brings different units into commensurable quantities. It also plays the role of preconditioner. Therefore, we opt to passing metric tensor explicitly instead of demanding existance of canonical one via Hilber space.","title":"Gradient descent"},{"location":"intro/part2/","text":"Linear computational graph One way to do reverse mode automatic differentiation is to create a DSL for differentiable functions and use StableName s to recover sharing information and construct computational graph. We adopt this approach, but with a twist. Source code Full source code of linear AST as explained in this section can be found here . The twist Derivative of function \\(f\\) is a linear function \\(f'\\) , which is the best local linear approximation of \\(f\\) at some point \\(x_0\\) . There\u2019s no need to construct a graph for the whole function \\(f\\) . Graph for \\(f'\\) is enough. Linear functions are much simpler than more general differentiable functions and lend much better to the Haskell type system, as we will see later. The idea is to start like in forward mode differentiation: data BVar a = BVar { bvarValue :: a , bvarGrad :: Expr a } Except that bvarGrad is not the value of the gradient, it\u2019s an abstract syntax tree of gradient\u2019s computation instead. This way bvarGrad is a linear expression, by construction. Building the graph for bvarGrad only (as opposed to bvarValue ) greatly reduces the scope and complexity of the otherwise tricky \u201creverse\u201d part of differentiation algorithm. Linear maps Automatic differentiation is all about vector spaces and linear maps. Let me quickly introduce them. Vector spaces are covered by vector-space package. Two most relevant operations are vector addition and scalar-vector multiplication: (^+^) :: v -> v -> v (*^) :: Scalar v -> v -> v Linear map is a mapping \\(f: U \\to V\\) , where \\(U\\) and \\(V\\) are vector spaces, satisfying the following conditions: \\[\\begin{align} f(x+y) & = f(x) + f(y) \\\\ f(a x) & = a f(x) \\end{align}\\] While linear maps are conceptually just functions, we can\u2019t represent all of them as Haskell functions, as that would lead to terrible algorithmic complexity. The best way to represent a linear map depends on the vector spaces in question and on the nature of linear map itself. Threfore we introduce a class for linear maps with an operator to evaluate them. The choice of operator comes from the fact that linear maps can be represented as matrices (or, more generally, tensors) and evaluation corresponds to matrix-vector product. class TensorMul u v where type u \u2715 v :: Type (\u2715) :: u -> v -> u \u2715 v If f represents linear map \\(U \\to V\\) and u :: U , then f \u2715 u :: V evaluates \\(f(u)\\) . Such a general operator wouldn\u2019t be very good for a Haskell library. More specific functions have better type inference, better error messages and make code easier to read and navigate. Operator \u2715 is supposed to mean tensor product followed by contraction, but there might be multiple sensible contractions, with no way to choose the right one at each call site. Anyway, it is very useful for explaining things and demonstrating that quite a few operations are actually the same. Laws of linear map can now be translated to Haskell: f \u2715 (u ^+^ v) = f \u2715 u ^+^ f \u2715 v f \u2715 (a *^ u) = a *^ (f \u2715 u) Multiplication \u2715 distributes over addition on the other side, too, because linear maps form a vector space themselves: (f ^+^ g) \u2715 u = f \u2715 u ^+^ g \u2715 u (a *^ f) \u2715 u = a *^ (f \u2715 u) A common case in backpropagation is domain of \\(f\\) being scalar. We will name it \\(\\mathbb{R}\\) to make this text more intuitive, though actual type of the scalar isn\u2019t really important. Gradient of variable \\(u \\in U\\) in this case is a linear map \\(u^*: U \\to \\mathbb{R}\\) . Vector space of such linear maps is said to be dual vector space of \\(U\\) . Translating this to Haskell and choosing name du for \\(u^*\\) gives u :: u du :: du du \u2715 u :: R We use lowercase type variables u and du , because all automatic differentiation code will be polymorphic \u2013 u and du are type variables. Going back to matrix analogy, if u is a column vectors, then du is a row vector and their product is a scalar. Here du can be seen not only as a (row) vector, but also as a function: (du \u2715) :: u -> R Vector u can be seen as a function, too: (\u2715 u) :: du -> R There\u2019s a nice symmetry between u and du \u2013 both have data representation, both have function representation and both are duals of each other. Another important operation besides evaluation is composition. We don\u2019t need another operator, because \u2715 fits the bill. If you see linear maps as matrices, composition is matrix multiplication. This usage of \u2715 gives rise to associativity law. Here are associative law of \u2715 together with the laws of usual Haskell function application and composition operators, put together to show relation between them: (f . g) $ u = f $ (g $ u) (f \u2715 g) \u2715 u = f \u2715 (g \u2715 u) (f . g) . h = f . (g . h) (f \u2715 g) \u2715 h = f \u2715 (g \u2715 h) PrimFunc The first ingredient of linear computational graphs is linear functions of a single argument. data PrimFunc u du v dv = PrimFunc { fwdFun :: u -> v , backFun :: dv -> du } PrimFunc is made of two parts: u -> v evaluates this function, while dv -> du backpropagates gradient. Given it\u2019s a linear map, it should have TensorMul instance, but unfortunately we quickly run into overlapping instances problem. We resort to newtype wrappers to overcome it. newtype Vec x = Vec { unVec :: x } This little nuisance is a consequence of overly general TensorMul class. The instance can now be given: instance TensorMul (PrimFunc u du v dv) (Vec u) where type (PrimFunc u du v dv) \u2715 (Vec u) = Vec v (PrimFunc f _) \u2715 Vec v = Vec (f v) That was forward mode evaluation. Can you guess which operator we\u2019re going to use for reverse mode? Of course, it has to be \u2715 . There\u2019s one more way to use it \u2013 on the left of the function: f \u2715 u :: v dv \u2715 f :: du Matrix analogy goes a long way here \u2013 if u and v are a column vectors, du and dv are row vectors, then f is a matrix and \u2715 is matrix-vector or vector-matrix multiplication. Another thing worth mentioning \u2013 there are no transpositions of matrices in sight. Matrix transposition assumes Hilbert space, we shouldn\u2019t be expecting them here. Since we already have newtype wrappers for vectors, we might create a different one for gradients. newtype Cov x = Cov {unCov :: x} Cov stands for covector . It doesn\u2019t have much to do with variance and covariance, it just indicates that the variable should be positioned on the left side of the function. instance TensorMul (Cov dv) (PrimFunc u du v dv) where type (Cov dv) \u2715 (PrimFunc u du v dv) = Cov du Cov v \u2715 (PrimFunc _ f) = Cov (f v) Function \\(f\\) can be seen as a bilinear form. If Haskell allowed such notation: (\u2715 f \u2715) :: dv -> u -> R Associative law comes into play again: dv \u2715 (f \u2715 u) = (dv \u2715 f) \u2715 u dv \u2715 fwdFun f u = backFun f dv \u2715 u This means fwdFun and backFun can\u2019t be arbitrary linear maps \u2013 above equation must hold for all choices of dv and u . Mathematically, this law says that backFun must be transpose of fwdFun. That\u2019s pretty much the definition of tranpose of a linear map. AST We are ready to start building our AST: data Expr a da v dv where Var :: Expr a da a da Func :: PrimFunc u du v dv -> Expr a da u du -> Expr a da v dv Expr a da v dv is a linear expression of type v with one free variable of type a . Linear functions with multiple arguments There\u2019s a difference between linear and bilinear (or multilinear) functions. Linear functions with two variables satisfy this equation: $$ f(x_1+x_2,y_1+y_2) = f(x_1, y_1) + f(x_2, y_2) $$ Multiplication, for example, is bilinear, not linear because $$ (a+b) \\cdot (x+y) \\ne a \\cdot x + b \\cdot y $$ Linearity is a much stronger restriction than multilinearity. It turns turns out any linear function can be written as a sum of one variable linear functions: $$ f(x_1, x_2, \u2026, x_n) = f_1(x_1) + f_2(x_2) + \\cdots + f_n(x_n) $$ for some \\(f_1\\) , \\(f_2\\) , \u2026, \\(f_n\\) . We have all the pieces to finish AST definition: data Expr a da v dv where Var :: Expr a da a da Func :: PrimFunc u du v dv -> Expr a da u du -> Expr a da v dv Sum :: AdditiveGroup v => [Expr a da v dv] -> Expr a da v dv Thats it! That\u2019s all we need to evaluate in reverse mode. Evaluation Evaluating Expr directly is inefficient \u2013 we should recover sharing information first. Anyways, let\u2019s see what needs to be evaluated first. Expr a da v dv represents a function a -> v , so it\u2019s natural to give it a TensorMul instance. The code writes itself: instance TensorMul (Expr a da v dv) (Vec a) where type Expr a da v dv \u2715 Vec a = Vec v expr \u2715 a = case expr of Var -> a -- Var is identity function Func f v -> f \u2715 (v \u2715 a) -- Func f v = f \u2715 v Sum vs -> sumV [v \u2715 a | v <- vs] Reverse mode evaluation is also straightforward: instance AdditiveGroup da => TensorMul (Vec dv) (Expr a da v dv) where type Vec dv \u2715 (Expr a da v dv) = Vec da dv \u2715 expr = case expr of Var -> dv Func f v -> (dv \u2715 f) \u2715 v Sum vs -> sumV [dv \u2715 v | v <- vs] Transposition While evaluation code is mechanical and boring, it gets more interesting with transposition. In order not to spoil the fun, it\u2019s left as a puzzle for the reader. transposeExpr :: AdditiveGroup da => Expr a da v dv -> Expr dv v da a transposeExpr = _ Can you fill in the hole? This time code doesn\u2019t write itself. At first I attacked this problem using type tetris approach myself, but that proved too hard and I had to reach for a pen and paper. You can see my solution here","title":"Linear Graph"},{"location":"intro/part2/#linear-computational-graph","text":"One way to do reverse mode automatic differentiation is to create a DSL for differentiable functions and use StableName s to recover sharing information and construct computational graph. We adopt this approach, but with a twist.","title":"Linear computational graph"},{"location":"intro/part2/#source-code","text":"Full source code of linear AST as explained in this section can be found here .","title":"Source code"},{"location":"intro/part2/#the-twist","text":"Derivative of function \\(f\\) is a linear function \\(f'\\) , which is the best local linear approximation of \\(f\\) at some point \\(x_0\\) . There\u2019s no need to construct a graph for the whole function \\(f\\) . Graph for \\(f'\\) is enough. Linear functions are much simpler than more general differentiable functions and lend much better to the Haskell type system, as we will see later. The idea is to start like in forward mode differentiation: data BVar a = BVar { bvarValue :: a , bvarGrad :: Expr a } Except that bvarGrad is not the value of the gradient, it\u2019s an abstract syntax tree of gradient\u2019s computation instead. This way bvarGrad is a linear expression, by construction. Building the graph for bvarGrad only (as opposed to bvarValue ) greatly reduces the scope and complexity of the otherwise tricky \u201creverse\u201d part of differentiation algorithm.","title":"The twist"},{"location":"intro/part2/#linear-maps","text":"Automatic differentiation is all about vector spaces and linear maps. Let me quickly introduce them. Vector spaces are covered by vector-space package. Two most relevant operations are vector addition and scalar-vector multiplication: (^+^) :: v -> v -> v (*^) :: Scalar v -> v -> v Linear map is a mapping \\(f: U \\to V\\) , where \\(U\\) and \\(V\\) are vector spaces, satisfying the following conditions: \\[\\begin{align} f(x+y) & = f(x) + f(y) \\\\ f(a x) & = a f(x) \\end{align}\\] While linear maps are conceptually just functions, we can\u2019t represent all of them as Haskell functions, as that would lead to terrible algorithmic complexity. The best way to represent a linear map depends on the vector spaces in question and on the nature of linear map itself. Threfore we introduce a class for linear maps with an operator to evaluate them. The choice of operator comes from the fact that linear maps can be represented as matrices (or, more generally, tensors) and evaluation corresponds to matrix-vector product. class TensorMul u v where type u \u2715 v :: Type (\u2715) :: u -> v -> u \u2715 v If f represents linear map \\(U \\to V\\) and u :: U , then f \u2715 u :: V evaluates \\(f(u)\\) . Such a general operator wouldn\u2019t be very good for a Haskell library. More specific functions have better type inference, better error messages and make code easier to read and navigate. Operator \u2715 is supposed to mean tensor product followed by contraction, but there might be multiple sensible contractions, with no way to choose the right one at each call site. Anyway, it is very useful for explaining things and demonstrating that quite a few operations are actually the same. Laws of linear map can now be translated to Haskell: f \u2715 (u ^+^ v) = f \u2715 u ^+^ f \u2715 v f \u2715 (a *^ u) = a *^ (f \u2715 u) Multiplication \u2715 distributes over addition on the other side, too, because linear maps form a vector space themselves: (f ^+^ g) \u2715 u = f \u2715 u ^+^ g \u2715 u (a *^ f) \u2715 u = a *^ (f \u2715 u) A common case in backpropagation is domain of \\(f\\) being scalar. We will name it \\(\\mathbb{R}\\) to make this text more intuitive, though actual type of the scalar isn\u2019t really important. Gradient of variable \\(u \\in U\\) in this case is a linear map \\(u^*: U \\to \\mathbb{R}\\) . Vector space of such linear maps is said to be dual vector space of \\(U\\) . Translating this to Haskell and choosing name du for \\(u^*\\) gives u :: u du :: du du \u2715 u :: R We use lowercase type variables u and du , because all automatic differentiation code will be polymorphic \u2013 u and du are type variables. Going back to matrix analogy, if u is a column vectors, then du is a row vector and their product is a scalar. Here du can be seen not only as a (row) vector, but also as a function: (du \u2715) :: u -> R Vector u can be seen as a function, too: (\u2715 u) :: du -> R There\u2019s a nice symmetry between u and du \u2013 both have data representation, both have function representation and both are duals of each other. Another important operation besides evaluation is composition. We don\u2019t need another operator, because \u2715 fits the bill. If you see linear maps as matrices, composition is matrix multiplication. This usage of \u2715 gives rise to associativity law. Here are associative law of \u2715 together with the laws of usual Haskell function application and composition operators, put together to show relation between them: (f . g) $ u = f $ (g $ u) (f \u2715 g) \u2715 u = f \u2715 (g \u2715 u) (f . g) . h = f . (g . h) (f \u2715 g) \u2715 h = f \u2715 (g \u2715 h)","title":"Linear maps"},{"location":"intro/part2/#primfunc","text":"The first ingredient of linear computational graphs is linear functions of a single argument. data PrimFunc u du v dv = PrimFunc { fwdFun :: u -> v , backFun :: dv -> du } PrimFunc is made of two parts: u -> v evaluates this function, while dv -> du backpropagates gradient. Given it\u2019s a linear map, it should have TensorMul instance, but unfortunately we quickly run into overlapping instances problem. We resort to newtype wrappers to overcome it. newtype Vec x = Vec { unVec :: x } This little nuisance is a consequence of overly general TensorMul class. The instance can now be given: instance TensorMul (PrimFunc u du v dv) (Vec u) where type (PrimFunc u du v dv) \u2715 (Vec u) = Vec v (PrimFunc f _) \u2715 Vec v = Vec (f v) That was forward mode evaluation. Can you guess which operator we\u2019re going to use for reverse mode? Of course, it has to be \u2715 . There\u2019s one more way to use it \u2013 on the left of the function: f \u2715 u :: v dv \u2715 f :: du Matrix analogy goes a long way here \u2013 if u and v are a column vectors, du and dv are row vectors, then f is a matrix and \u2715 is matrix-vector or vector-matrix multiplication. Another thing worth mentioning \u2013 there are no transpositions of matrices in sight. Matrix transposition assumes Hilbert space, we shouldn\u2019t be expecting them here. Since we already have newtype wrappers for vectors, we might create a different one for gradients. newtype Cov x = Cov {unCov :: x} Cov stands for covector . It doesn\u2019t have much to do with variance and covariance, it just indicates that the variable should be positioned on the left side of the function. instance TensorMul (Cov dv) (PrimFunc u du v dv) where type (Cov dv) \u2715 (PrimFunc u du v dv) = Cov du Cov v \u2715 (PrimFunc _ f) = Cov (f v) Function \\(f\\) can be seen as a bilinear form. If Haskell allowed such notation: (\u2715 f \u2715) :: dv -> u -> R Associative law comes into play again: dv \u2715 (f \u2715 u) = (dv \u2715 f) \u2715 u dv \u2715 fwdFun f u = backFun f dv \u2715 u This means fwdFun and backFun can\u2019t be arbitrary linear maps \u2013 above equation must hold for all choices of dv and u . Mathematically, this law says that backFun must be transpose of fwdFun. That\u2019s pretty much the definition of tranpose of a linear map.","title":"PrimFunc"},{"location":"intro/part2/#ast","text":"We are ready to start building our AST: data Expr a da v dv where Var :: Expr a da a da Func :: PrimFunc u du v dv -> Expr a da u du -> Expr a da v dv Expr a da v dv is a linear expression of type v with one free variable of type a .","title":"AST"},{"location":"intro/part2/#linear-functions-with-multiple-arguments","text":"There\u2019s a difference between linear and bilinear (or multilinear) functions. Linear functions with two variables satisfy this equation: $$ f(x_1+x_2,y_1+y_2) = f(x_1, y_1) + f(x_2, y_2) $$ Multiplication, for example, is bilinear, not linear because $$ (a+b) \\cdot (x+y) \\ne a \\cdot x + b \\cdot y $$ Linearity is a much stronger restriction than multilinearity. It turns turns out any linear function can be written as a sum of one variable linear functions: $$ f(x_1, x_2, \u2026, x_n) = f_1(x_1) + f_2(x_2) + \\cdots + f_n(x_n) $$ for some \\(f_1\\) , \\(f_2\\) , \u2026, \\(f_n\\) . We have all the pieces to finish AST definition: data Expr a da v dv where Var :: Expr a da a da Func :: PrimFunc u du v dv -> Expr a da u du -> Expr a da v dv Sum :: AdditiveGroup v => [Expr a da v dv] -> Expr a da v dv Thats it! That\u2019s all we need to evaluate in reverse mode.","title":"Linear functions with multiple arguments"},{"location":"intro/part2/#evaluation","text":"Evaluating Expr directly is inefficient \u2013 we should recover sharing information first. Anyways, let\u2019s see what needs to be evaluated first. Expr a da v dv represents a function a -> v , so it\u2019s natural to give it a TensorMul instance. The code writes itself: instance TensorMul (Expr a da v dv) (Vec a) where type Expr a da v dv \u2715 Vec a = Vec v expr \u2715 a = case expr of Var -> a -- Var is identity function Func f v -> f \u2715 (v \u2715 a) -- Func f v = f \u2715 v Sum vs -> sumV [v \u2715 a | v <- vs] Reverse mode evaluation is also straightforward: instance AdditiveGroup da => TensorMul (Vec dv) (Expr a da v dv) where type Vec dv \u2715 (Expr a da v dv) = Vec da dv \u2715 expr = case expr of Var -> dv Func f v -> (dv \u2715 f) \u2715 v Sum vs -> sumV [dv \u2715 v | v <- vs]","title":"Evaluation"},{"location":"intro/part2/#transposition","text":"While evaluation code is mechanical and boring, it gets more interesting with transposition. In order not to spoil the fun, it\u2019s left as a puzzle for the reader. transposeExpr :: AdditiveGroup da => Expr a da v dv -> Expr dv v da a transposeExpr = _ Can you fill in the hole? This time code doesn\u2019t write itself. At first I attacked this problem using type tetris approach myself, but that proved too hard and I had to reach for a pen and paper. You can see my solution here","title":"Transposition"},{"location":"intro/part3/","text":"Sparsity It\u2019s easy to run into quadratic complexity. Gradients are often sparse. Consider backpropagating over fst :: (a, b) -> a . If gradient of a is da , then gradient of (a, b) is (da, zeroV) . The second element is just a zero, but it might be a very fat zero \u2013 maybe a large nested record of big arrays, all full of zeros! Extreme case of gradient sparsity appears in indexing a vector. All but one elements of gradient are zero. Constructing such a vector makes indexing \\(O(n)\\) operation. Sparse gradients Imperative implementations of backpropagation dodge this problem by updating gradients in-place. While that\u2019s possible to do in Haskell, there\u2019s a better way \u2013 builders. Builder is data type for efficient representation of sparse gradients. Or it might be a ST action that bumps the gradient in-place if you wish. Downhill library has a class for builders: class Monoid (VecBuilder v) => BasicVector v where type VecBuilder v :: Type sumBuilder :: VecBuilder v -> v BasicVector is absolutely minimal requirement for a type to be eligible to automatic differentiation. Functions on graph edges produce builders. Nodes then mconcat them and pass to sumBuilder . For example, builder for pairs looks like this: type VecBuilder (a, b) = Maybe (VecBuilder a, VecBuilder b) Nothing stands for zero vector. Maybe is important here, mempty wouldn\u2019t be cheap for deeply nested pairs otherwise. Better AST Expr type in the library is different to that in previous part in a few ways. First of all, it hasn\u2019t got pairs of vectors and gradients, such as a da v dv . Just a v . Two sets of parameters allows both forward and reverse mode evaluation, but we do reverse mode only here. Those would be da dv for reverse mode. We drop superfluous \u201cd\u201d and call them a anv v . There\u2019s also a little problem with our Expr type. As we\u2019re going to convert it to a graph, we need a clear separation between nodes and edges. Func is definetely an edge. Sum itself is a node, but it contains a mixed bag of adjacent edges and nodes . We disallow this situation of nodes adjacent to nodes by splitting AST into terms and expressions: data Term a v where Term :: (v -> VecBuilder u) -> Expr a u -> Term a v data Expr a v where ExprVar :: Expr a a ExprSum :: BasicVector v => [Term a v] -> Expr a v There\u2019s v -> VecBuilder u in place of PrimFunc , which adds support for sparse gradients and drops forward mode evaluation. Also BasicVector replaces AdditiveGroup in ExprSum . Inline nodes Builders are not enough. Say, we have a simple newtype wrapper for vector. Let\u2019s have a closer look at what happens when we attempt to index it: newtype MyVector a = MyVector { unMyVector :: Vector a } myLookup :: MyVector a -> Int -> a myLookup v i = unMyVector v ! i If this code would be bluntly adapted to work on Expr , the tree would have three nodes with two edges between them: graph TD nodeMyVector[MyVector a] nodeVector[Vector a] nodeItem[a] nodeVector-- \"(! i)\" -->nodeItem nodeMyVector-- unMyVector -->nodeVector nodeMore[\"...\"] style nodeMore stroke:none,fill:none nodeMyVector-- \"...\" --> nodeMore Let\u2019s see how gradients propagate when we flip edges: graph BT nodeMyVectorB[\"Grad (MyVector a)\"] nodeMyVectorB1([\"GradBuilder (MyVector a)\"]) style nodeMyVectorB1 fill:white,stroke-dasharray: 5 5 nodeMore[\"...\"] style nodeMore stroke:none,fill:none nodeVectorB[\"<b>Grad (Vector a)</b>\"] nodeVectorB1([\"GradBuilder (Vector a)\"]) style nodeVectorB1 fill:white,stroke-dasharray: 5 5 nodeItemB[\"Grad a\"] nodeItemB-- \"(! i)\" -->nodeVectorB1 nodeVectorB1-- sumBuilder -->nodeVectorB nodeVectorB-- unMyVector -->nodeMyVectorB1 nodeMyVectorB1-- sumBuilder -->nodeMyVectorB nodeMore-- \"...\" -->nodeMyVectorB1 Indexing function (! i) produces lightweight gradient builder, as desired. Only for the intermediate node (labeled in bold font) to convert it into a big fat vector, undoing all optimization! BackGrad has the ability to relay gradients without summing them: newtype BackGrad a v = BackGrad ( forall x. (x -> VecBuilder v) -> Term a x ) BackGrad turns linear functions ( x -> VecBuilder v ) to Term s. Alternatively, you could see it as a Term data constructor with a hole in place of Expr argument. It generalizes Expr : realNode :: Expr a v -> BackGrad a v realNode x = BackGrad (\\f -> Term f x) and provides means to apply linear function without creating a node: inlineNode :: forall r u v. (VecBuilder v -> VecBuilder u) -> BackGrad r u -> BackGrad r v inlineNode f (BackGrad g) = BackGrad go where go :: forall x. (x -> VecBuilder v) -> Term r x go h = g (f . h) graph BT nodeMyVectorB[\"Grad (MyVector a)\"] nodeMyVectorB1([\"GradBuilder (MyVector a)\"]) nodeMore[\"...\"] style nodeMore fill:none,stroke:none style nodeMyVectorB1 fill:white,stroke-dasharray: 5 5 nodeVectorB1([\"GradBuilder (Vector a)\"]) style nodeVectorB1 fill:white,stroke-dasharray: 5 5 nodeItemB[\"Grad a\"] nodeItemB-- \"(! i)\" -->nodeVectorB1 nodeVectorB1-- unMyVector -->nodeMyVectorB1 nodeMyVectorB1-- sumBuilder -->nodeMyVectorB nodeMore-- \"...\" -->nodeMyVectorB1 Node that Expr is a node, Term is an edge. No Expr \u2013 no node. Sparse nodes Inline nodes are still not enough. There\u2019s no good way to access members of tuples, or other product types for that matter. They are important, because this library differentiates unary functions BVar a -> BVar b only. If we have many variables to differentiate with respect to, we have to pack them together into single tuple or record BVar a . For a complex model a might be a big structure of nested records. Automatic differentiation starts with a single big variable containing all the data and there has to be an efficient way to access all parts of it. Constructing real Expr nodes won\u2019t cut, because they lose sparsity and make the cost of accessing any member proportional to the size of the whole structure. Inline nodes are not an option, too. Accessing deeply nested members would create a long chain of inlineNode s. The cost of traversing the whole chain will have to be paid every time the variable is used. This way a simple traversal of a list will turn into into a Schlemiel the painter\u2019s algorithm! The solution is to store sparse gradients in graph nodes for this use case. Luckily, there\u2019s no need for new types of node here. Have a look at BackGrad definition \u2013 there\u2019s no v , only VecBuilder . This means we can choose a different type of node to store gradient and hide it under BackGrad as if nothing happened. No one can possibly notice. castBackGrad :: forall r v z. VecBuilder z ~ VecBuilder v => BackGrad r v -> BackGrad r z castBackGrad (BackGrad g) = BackGrad g Sparse gradients are wrapped in SparseVector newtype for storage in graph. Storing naked VecBuilder v runs into a little problem \u2013 what\u2019s VecBuilder (VecBuilder v) ? newtype SparseVector v = SparseVector { unSparseVector :: VecBuilder v } sumBuilder :: VecBuilder v -> SparseVector v doesn\u2019t really sum anything, it just stores unevaluated builders. How does it differ from inline nodes? Turns out monoid operation of builders of product types plays a key role in intermediate nodes. It collects gradients from all successor nodes and packs them into a tuple/record before passing them to parent node as a single unit. This way gradients are assembled bottom up into a tree of the same shape as original data. Inline nodes would propagate gradients form each leaf node all the way to the root individually.","title":"Sparsity"},{"location":"intro/part3/#sparsity","text":"It\u2019s easy to run into quadratic complexity. Gradients are often sparse. Consider backpropagating over fst :: (a, b) -> a . If gradient of a is da , then gradient of (a, b) is (da, zeroV) . The second element is just a zero, but it might be a very fat zero \u2013 maybe a large nested record of big arrays, all full of zeros! Extreme case of gradient sparsity appears in indexing a vector. All but one elements of gradient are zero. Constructing such a vector makes indexing \\(O(n)\\) operation.","title":"Sparsity"},{"location":"intro/part3/#sparse-gradients","text":"Imperative implementations of backpropagation dodge this problem by updating gradients in-place. While that\u2019s possible to do in Haskell, there\u2019s a better way \u2013 builders. Builder is data type for efficient representation of sparse gradients. Or it might be a ST action that bumps the gradient in-place if you wish. Downhill library has a class for builders: class Monoid (VecBuilder v) => BasicVector v where type VecBuilder v :: Type sumBuilder :: VecBuilder v -> v BasicVector is absolutely minimal requirement for a type to be eligible to automatic differentiation. Functions on graph edges produce builders. Nodes then mconcat them and pass to sumBuilder . For example, builder for pairs looks like this: type VecBuilder (a, b) = Maybe (VecBuilder a, VecBuilder b) Nothing stands for zero vector. Maybe is important here, mempty wouldn\u2019t be cheap for deeply nested pairs otherwise.","title":"Sparse gradients"},{"location":"intro/part3/#better-ast","text":"Expr type in the library is different to that in previous part in a few ways. First of all, it hasn\u2019t got pairs of vectors and gradients, such as a da v dv . Just a v . Two sets of parameters allows both forward and reverse mode evaluation, but we do reverse mode only here. Those would be da dv for reverse mode. We drop superfluous \u201cd\u201d and call them a anv v . There\u2019s also a little problem with our Expr type. As we\u2019re going to convert it to a graph, we need a clear separation between nodes and edges. Func is definetely an edge. Sum itself is a node, but it contains a mixed bag of adjacent edges and nodes . We disallow this situation of nodes adjacent to nodes by splitting AST into terms and expressions: data Term a v where Term :: (v -> VecBuilder u) -> Expr a u -> Term a v data Expr a v where ExprVar :: Expr a a ExprSum :: BasicVector v => [Term a v] -> Expr a v There\u2019s v -> VecBuilder u in place of PrimFunc , which adds support for sparse gradients and drops forward mode evaluation. Also BasicVector replaces AdditiveGroup in ExprSum .","title":"Better AST"},{"location":"intro/part3/#inline-nodes","text":"Builders are not enough. Say, we have a simple newtype wrapper for vector. Let\u2019s have a closer look at what happens when we attempt to index it: newtype MyVector a = MyVector { unMyVector :: Vector a } myLookup :: MyVector a -> Int -> a myLookup v i = unMyVector v ! i If this code would be bluntly adapted to work on Expr , the tree would have three nodes with two edges between them: graph TD nodeMyVector[MyVector a] nodeVector[Vector a] nodeItem[a] nodeVector-- \"(! i)\" -->nodeItem nodeMyVector-- unMyVector -->nodeVector nodeMore[\"...\"] style nodeMore stroke:none,fill:none nodeMyVector-- \"...\" --> nodeMore Let\u2019s see how gradients propagate when we flip edges: graph BT nodeMyVectorB[\"Grad (MyVector a)\"] nodeMyVectorB1([\"GradBuilder (MyVector a)\"]) style nodeMyVectorB1 fill:white,stroke-dasharray: 5 5 nodeMore[\"...\"] style nodeMore stroke:none,fill:none nodeVectorB[\"<b>Grad (Vector a)</b>\"] nodeVectorB1([\"GradBuilder (Vector a)\"]) style nodeVectorB1 fill:white,stroke-dasharray: 5 5 nodeItemB[\"Grad a\"] nodeItemB-- \"(! i)\" -->nodeVectorB1 nodeVectorB1-- sumBuilder -->nodeVectorB nodeVectorB-- unMyVector -->nodeMyVectorB1 nodeMyVectorB1-- sumBuilder -->nodeMyVectorB nodeMore-- \"...\" -->nodeMyVectorB1 Indexing function (! i) produces lightweight gradient builder, as desired. Only for the intermediate node (labeled in bold font) to convert it into a big fat vector, undoing all optimization! BackGrad has the ability to relay gradients without summing them: newtype BackGrad a v = BackGrad ( forall x. (x -> VecBuilder v) -> Term a x ) BackGrad turns linear functions ( x -> VecBuilder v ) to Term s. Alternatively, you could see it as a Term data constructor with a hole in place of Expr argument. It generalizes Expr : realNode :: Expr a v -> BackGrad a v realNode x = BackGrad (\\f -> Term f x) and provides means to apply linear function without creating a node: inlineNode :: forall r u v. (VecBuilder v -> VecBuilder u) -> BackGrad r u -> BackGrad r v inlineNode f (BackGrad g) = BackGrad go where go :: forall x. (x -> VecBuilder v) -> Term r x go h = g (f . h) graph BT nodeMyVectorB[\"Grad (MyVector a)\"] nodeMyVectorB1([\"GradBuilder (MyVector a)\"]) nodeMore[\"...\"] style nodeMore fill:none,stroke:none style nodeMyVectorB1 fill:white,stroke-dasharray: 5 5 nodeVectorB1([\"GradBuilder (Vector a)\"]) style nodeVectorB1 fill:white,stroke-dasharray: 5 5 nodeItemB[\"Grad a\"] nodeItemB-- \"(! i)\" -->nodeVectorB1 nodeVectorB1-- unMyVector -->nodeMyVectorB1 nodeMyVectorB1-- sumBuilder -->nodeMyVectorB nodeMore-- \"...\" -->nodeMyVectorB1 Node that Expr is a node, Term is an edge. No Expr \u2013 no node.","title":"Inline nodes"},{"location":"intro/part3/#sparse-nodes","text":"Inline nodes are still not enough. There\u2019s no good way to access members of tuples, or other product types for that matter. They are important, because this library differentiates unary functions BVar a -> BVar b only. If we have many variables to differentiate with respect to, we have to pack them together into single tuple or record BVar a . For a complex model a might be a big structure of nested records. Automatic differentiation starts with a single big variable containing all the data and there has to be an efficient way to access all parts of it. Constructing real Expr nodes won\u2019t cut, because they lose sparsity and make the cost of accessing any member proportional to the size of the whole structure. Inline nodes are not an option, too. Accessing deeply nested members would create a long chain of inlineNode s. The cost of traversing the whole chain will have to be paid every time the variable is used. This way a simple traversal of a list will turn into into a Schlemiel the painter\u2019s algorithm! The solution is to store sparse gradients in graph nodes for this use case. Luckily, there\u2019s no need for new types of node here. Have a look at BackGrad definition \u2013 there\u2019s no v , only VecBuilder . This means we can choose a different type of node to store gradient and hide it under BackGrad as if nothing happened. No one can possibly notice. castBackGrad :: forall r v z. VecBuilder z ~ VecBuilder v => BackGrad r v -> BackGrad r z castBackGrad (BackGrad g) = BackGrad g Sparse gradients are wrapped in SparseVector newtype for storage in graph. Storing naked VecBuilder v runs into a little problem \u2013 what\u2019s VecBuilder (VecBuilder v) ? newtype SparseVector v = SparseVector { unSparseVector :: VecBuilder v } sumBuilder :: VecBuilder v -> SparseVector v doesn\u2019t really sum anything, it just stores unevaluated builders. How does it differ from inline nodes? Turns out monoid operation of builders of product types plays a key role in intermediate nodes. It collects gradients from all successor nodes and packs them into a tuple/record before passing them to parent node as a single unit. This way gradients are assembled bottom up into a tree of the same shape as original data. Inline nodes would propagate gradients form each leaf node all the way to the root individually.","title":"Sparse nodes"}]}